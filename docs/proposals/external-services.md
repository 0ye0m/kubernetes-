#Kubernetes External Services

## Problem Definition

To make a Kubernetes service, say a web server, with service port 80, externally available on the internet, the following steps are required:

On GCE:

1. Create firewall rules that open port 80 on each node of the cluster to the internet.
1. Create a Load Balancer static IP (public).
1. Create forwarding rule from the static_IP:80 to port 80 on each node.

The first step (required on GCE), has the undesirable effect that the port on each node is exposed on the internet since nodes have public ips(possible ephemeral). A user can directly hit the node's public ip:80. 
Any private process running on the node on port 80, is now additonally exposed to the internet. This is unintentional.
The original goal was to only expose the kubernetes service on the internet via the Load Balancer.

## Proposal

### CloudProvider Pod
Each cluster will have a pod that has the following responsibilities:

1. Gather cloud-provider-specific metadata like fetch firewall rules etc. 
1. Execute cloud-provider-specific commands like create firewall rules etc.
1. Determine node-specific commands that need to be run on each node.
1. Via http: expose an endpoint for nodes to retrieve(GET) the commands they need to be run.
1. Via http: expose an enpoint for nodes to submit(POST) user-intent like create an externally-exposed service.

The CloudProvider Pod is an addon and will be different for each cloud-provider.
The endpoints that the pod exposes will have a common RESTful API. This allows cloud-provider specific code to be limited to the CloudProvider Pod while allowing the cluster to be cloud-provider agnostic.

### Solving the GCE External Service problem

We need to separate firewall rules that were "created by the user" v/s "created by kubernetes to expose services". 
For ports that were opened only to expose services, we will add iptable INPUT/POSTROUTING rules to drop packets that arrive on the port with packet.destIP != serviceLoadBalancer.IP. We will have to make sure that all unintented traffic gets blocked by this, including the case where we have containers on the node running with hostPort = servicePort. The advantage of using INPUT/POSTROUTING rules is that, kube-proxy does not use them.

1. Node informs GCEProvider Pod of intent to have external-service-ip.
1. The GCEProvider Pod will have the ability to create firewall rules to open service ports. Firewall rules will be created with some name like "Kubernetes-AutoGenerated-ServiceName-PortNumber".
1. GCEProvider Pod will get all firewall rules that exist for the project.
1. For ports that are meant only for the service, it will create commands for each node to execute.
1. Each node will query GCEProvider POD to determine what commands it needs to run.
1. Each node runs those commands (for e.g. iptables -A INPUT -p tcp --dport 80 -d != serviceLB.IP -j DROP)

TBD: Which process in the node is responsible for querying the CloudProvider Pod for getting the node-specic commands ? Is it the kubelet ? or a seperate daemon ?

## Advantages
Having a CloudProvider Pod allows us to port kubernetes cluster to various different cloud-providers easily, simply by creating a new CloudProvider-Specific Pod.




[![Analytics](https://kubernetes-site.appspot.com/UA-36037335-10/GitHub/docs/proposals/federation.md?pixel)]()
