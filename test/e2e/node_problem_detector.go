/*
Copyright 2016 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package e2e

import (
	"fmt"
	"strconv"
	"time"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/fields"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/kubernetes/pkg/api/v1"
	"k8s.io/kubernetes/pkg/client/clientset_generated/clientset"
	coreclientset "k8s.io/kubernetes/pkg/client/clientset_generated/clientset/typed/core/v1"
	"k8s.io/kubernetes/test/e2e/framework"

	. "github.com/onsi/ginkgo"
	. "github.com/onsi/gomega"
)

// FIXME: Push and update image name.
const kernelLogGeneratorImage = "gcr.io/google.com/noogler-kubernetes/kernel_log_generator:0.1"

var _ = framework.KubeDescribe("NodeProblemDetector [Serial] [Disruptive] [Feature:NodeProblemDetector]", func() {
	const (
		pollInterval   = 1 * time.Second
		pollConsistent = 5 * time.Second
		pollTimeout    = 3 * time.Minute

		// There is no namespace for Node, event recorder will set default namespace for node events.
		eventNamespace = metav1.NamespaceDefault

		source                  = "kernel-monitor"
		conditionType           = "KernelDeadlock"
		defaultConditionReason  = "KernelHasNoDeadlock"
		defaultConditionMessage = "kernel has no deadlock"
	)
	f := framework.NewDefaultFramework("node-problem-detector")
	var c clientset.Interface
	var nodeClient coreclientset.NodeInterface
	var eventClient coreclientset.EventInterface
	BeforeEach(func() {
		c = f.ClientSet
		nodeClient = c.Core().Nodes()
		eventClient = c.Core().Events(eventNamespace)
	})

	// Test system log monitor deployed in the cluster. We may add other tests if we have more problem daemons in the future.
	framework.KubeDescribe("SystemLogMonitor", func() {
		var nodeName string
		var eventListOptions metav1.ListOptions
		// problems is a map of a problems used in the test. These problems are built in the kernel log generator.
		// Note that the problems used in this test must be matched by system log monitor with one-line pattern,
		// because multi-line logs generated by kernel log generator may be broken by original kernel log generated
		// by the node.
		problems := map[string]struct {
			problem         string
			eventReason     string
			conditionReason string
		}{
			"normal": {
				problem: "au_opts_verify",
			},
			"notable": {
				problem:     "unregister_netdevice",
				eventReason: "UnregisterNetDevice",
			},
			"fatal": {
				problem:         "docker_hung",
				eventReason:     "TaskHung",
				conditionReason: "DockerHung",
			},
		}

		cleanupNode := func(nodeName string) {
			if err := framework.VerifyNodeCondition(nodeClient, nodeName, conditionType, v1.ConditionFalse, defaultConditionReason, defaultConditionMessage); err != nil {
				By("Reboot the node to reset node condition")
				Expect(f.RestartNodes([]string{nodeName})).To(Succeed())
				By("Wait for the node to be ready")
				Expect(framework.WaitForNodeToBeReady(c, nodeName, framework.RestartNodeReadyAgainTimeout)).To(BeTrue())
			}
			if err := framework.VerifyEvents(eventClient, eventListOptions, 0, "", ""); err != nil {
				By("Cleanup existing node events")
				Expect(c.Core().Events(eventNamespace).DeleteCollection(metav1.NewDeleteOptions(0), eventListOptions)).To(Succeed())
			}
			By("Make sure default node condition is generated")
			framework.EventuallyAndConsistently(func() error {
				return framework.VerifyNodeCondition(nodeClient, nodeName, conditionType, v1.ConditionFalse, defaultConditionReason, defaultConditionMessage)
			}, pollTimeout, pollConsistent, pollInterval)
			By("Make sure no events are generated")
			framework.EventuallyAndConsistently(func() error {
				return framework.VerifyEvents(eventClient, eventListOptions, 0, "", "")
			}, pollTimeout, pollConsistent, pollInterval)
		}

		BeforeEach(func() {
			// The RestartNodes function used in this test only support gce and gke.
			framework.SkipUnlessProviderIs("gce", "gke")
			// Actually, the test should work with all os distro. However, currently kernel log generator only works
			// with journald. So limit the test node os distro as gci now.
			// TODO(random-liu): Update kernel log generator to support syslog and enable the test in non-gci images.
			framework.SkipUnlessNodeOSDistroIs("gci")

			By("Get a ready and schedulable node")
			nodes := framework.GetReadySchedulableNodesOrDie(c)
			Expect(nodes.Items).NotTo(BeEmpty())
			nodeName = nodes.Items[0].Name
			eventListOptions = metav1.ListOptions{
				FieldSelector: fields.Set{
					"involvedObject.kind":      "Node",
					"involvedObject.name":      nodeName,
					"involvedObject.namespace": metav1.NamespaceAll,
					"source":                   source,
				}.AsSelector().String(),
			}

			// Wait for the kernel deadlock condition to show up.
			Eventually(func() error {
				_, err := framework.GetNodeCondition(nodeClient, nodeName, conditionType)
				return err
			}, pollTimeout, pollInterval).Should(Succeed())
			// Cleanup the existing node condition and events if there are any.
			cleanupNode(nodeName)
		})

		It("should not update anything for normal log", func() {
			// Note that lines per second is mainly for performance benchmark test. For run once mode, it
			// doesn't matter. So just set a arbitrary large value.
			k := getKernelLogGenerator(nodeName, problems["normal"].problem, true, 1000)
			f.PodClient().Create(k)
			f.PodClient().WaitForSuccess(k.Name, pollTimeout)
			By("Make sure no events are generated")
			framework.EventuallyAndConsistently(func() error {
				return framework.VerifyEvents(eventClient, eventListOptions, 0, "", "")
			}, pollTimeout, pollConsistent, pollInterval)
			By("Make sure node condition is not changed")
			framework.EventuallyAndConsistently(func() error {
				return framework.VerifyNodeCondition(nodeClient, nodeName, conditionType, v1.ConditionFalse, defaultConditionReason, defaultConditionMessage)
			}, pollTimeout, pollConsistent, pollInterval)
		})

		It("should generate events for notable error", func() {
			k := getKernelLogGenerator(nodeName, problems["notable"].problem, true, 1000)
			f.PodClient().Create(k)
			f.PodClient().WaitForSuccess(k.Name, pollTimeout)
			By("Make sure 1 event is generated")
			framework.EventuallyAndConsistently(func() error {
				return framework.VerifyEvents(eventClient, eventListOptions, 1, problems["notable"].eventReason, "")
			}, pollTimeout, pollConsistent, pollInterval)
			By("Make sure node condition is not changed")
			framework.EventuallyAndConsistently(func() error {
				return framework.VerifyNodeCondition(nodeClient, nodeName, conditionType, v1.ConditionFalse, defaultConditionReason, defaultConditionMessage)
			}, pollTimeout, pollConsistent, pollInterval)
		})

		It("should update node condition for fatal error", func() {
			k := getKernelLogGenerator(nodeName, problems["fatal"].problem, true, 1000)
			f.PodClient().Create(k)
			f.PodClient().WaitForSuccess(k.Name, pollTimeout)
			By("Make sure 1 event is generated")
			framework.EventuallyAndConsistently(func() error {
				return framework.VerifyEvents(eventClient, eventListOptions, 1, problems["fatal"].eventReason, "")
			}, pollTimeout, pollConsistent, pollInterval)
			By("Make sure node condition is set")
			framework.EventuallyAndConsistently(func() error {
				return framework.VerifyNodeCondition(nodeClient, nodeName, conditionType, v1.ConditionTrue, problems["fatal"].conditionReason, "")
			}, pollTimeout, pollConsistent, pollInterval)
			By("Reboot the node to reset node condition")
			Expect(f.RestartNodes([]string{nodeName})).To(Succeed())
			By("Wait for the node to be ready")
			Expect(framework.WaitForNodeToBeReady(c, nodeName, framework.RestartNodeReadyAgainTimeout)).To(BeTrue())
			By("Make sure node condition is reset after reboot")
			framework.EventuallyAndConsistently(func() error {
				return framework.VerifyNodeCondition(nodeClient, nodeName, conditionType, v1.ConditionFalse, defaultConditionReason, defaultConditionMessage)
			}, pollTimeout, pollConsistent, pollInterval)
		})

		It("should reconcile node condition if it is changed", func() {
			k := getKernelLogGenerator(nodeName, problems["fatal"].problem, true, 1000)
			f.PodClient().Create(k)
			f.PodClient().WaitForSuccess(k.Name, pollTimeout)
			By("Make sure 1 event is generated")
			framework.EventuallyAndConsistently(func() error {
				return framework.VerifyEvents(eventClient, eventListOptions, 1, problems["fatal"].eventReason, "")
			}, pollTimeout, pollConsistent, pollInterval)
			By("Make sure node condition is set")
			framework.EventuallyAndConsistently(func() error {
				return framework.VerifyNodeCondition(nodeClient, nodeName, conditionType, v1.ConditionTrue, problems["fatal"].conditionReason, "")
			}, pollTimeout, pollConsistent, pollInterval)
			By("Remove the node condition")
			patch := []byte(fmt.Sprintf(`{"status":{"conditions":[{"$patch":"delete","type":"%s"}]}}`, conditionType))
			c.Core().RESTClient().Patch(types.StrategicMergePatchType).Resource("nodes").Name(nodeName).SubResource("status").Body(patch).Do()
			By("Make sure node condition is recovered")
			framework.EventuallyAndConsistently(func() error {
				return framework.VerifyNodeCondition(nodeClient, nodeName, conditionType, v1.ConditionTrue, problems["fatal"].conditionReason, "")
			}, pollTimeout, pollConsistent, pollInterval)
		})

		AfterEach(func() {
			cleanupNode(nodeName)
		})
	})
})

func getKernelLogGenerator(nodeName, problem string, runOnce bool, linesPerSecond int) *v1.Pod {
	isPrivileged := true
	return &v1.Pod{
		ObjectMeta: metav1.ObjectMeta{
			Name: "kernel-log-generator",
		},
		Spec: v1.PodSpec{
			NodeName:      nodeName,
			RestartPolicy: v1.RestartPolicyOnFailure,
			Containers: []v1.Container{
				{
					Name:  "kernel-log-generator",
					Image: kernelLogGeneratorImage,
					Env: []v1.EnvVar{
						{
							Name:  "PROBLEM",
							Value: fmt.Sprintf("/problems/%s", problem),
						},
						{
							Name:  "RUN_ONCE",
							Value: strconv.FormatBool(runOnce),
						},
						{
							Name:  "LINES_PER_SECOND",
							Value: strconv.Itoa(linesPerSecond),
						},
					},
					SecurityContext: &v1.SecurityContext{Privileged: &isPrivileged},
				},
			},
		},
	}
}
